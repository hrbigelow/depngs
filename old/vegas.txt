The Vegas routine is an iterative function depending on a state from
one call to the next.  The state records the sizes of bins and boxes
in a histogram meant to approximate the density of the function along
its marginals.  In order for the approximation to converge to its
limits of accuracy (determined by the function and the number of calls
permitted) it must be called enough times for the approximations of
the marginals to serve well enough to approximate the other marginals
in a sort-of bootstrapping process.  Once this has happened, though,
the routine can be run again to generate independent estimates of the
mean and variance.  (is this true?)

The chi-squared estimate depends on the number of calls used to create
the grid, and the degree to which the grid has converged in all
dimensions in bootstrap style.  Fortunately, the information gained
about the function from previous rounds can be preserved by using
'stage = 2'.  It is unclear whether the weighted average should or
should not be re-calculated for this purpose.

Ultimately, the stopping criterion for estimation should be the
convergence of the values of specifically bounded integrals.  This
means the chi-squared test might have to be engineered for these
partial sums.

What though is the relationship between the chi-squared estimates of
the variance between grid box densities and the variance on the
estimate of the cutoff parameter itself?

It might be difficult to co-opt the vegas routine to give a
chi-squared calculation of a bounded integral, rather than the full
integral.

The main problem is that the relationship between the number of
function calls needed for *one* "good" estimate of a bounded integral
and the variance of many such estimates is unknown.

The only thing that seems possible is to choose some conservatively
low value for the total error.  Or, is it possible to calculate the
estimates only, using a different function but the same grid?

To combat efficiency issues, the function calls will be memoized.


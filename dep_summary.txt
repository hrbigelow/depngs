The other project I want to discuss is called 'dep' (diversity estimation
probabilistically), which I implemented while at the Broad Institute and
further developed at Amgen.  The replicating virus or cancer genome can mutate
at each division, with the result that a population is non-clonal.  Each
position in the genome potentially has a different selective or mutational
pressure.  Taking the population as a whole, we want to measure the fraction of
the population having A, C, G, and T at each position in the genome.  Knowing
this allows biologists to measure the effect of a drug on the mutational
profile, or to spot locations that correlate with survival or death.   

While cancer cells divide and mutate, the immune system recognizes some of them
as bad, and is able to kill them off.  Other cells acquire mutations that allow
them to evade attack for awhile until the immune system adapts.  A Darwinian
natural selection dynamic takes hold.  In between adaptations, a mostly
decimated population of cancer cells can regrow from a very small subpopulation
('clonal expansion'). 

Think of the game like this:  The immune system is AlphaGo playing a billion Go
games simultaneously, each against a different amateur.  At each time step, the
amateurs each make a random move.  AlphaGo responds and the process repeats.
Also, at each time step, some fraction of games end, with the amateurs losing.
When these games end, there is room for any remaining amateurs to clone
themselves with the same board state until there are a billion games in play. 

So, at any given time, in a very rare cases, the amateur might make a really
good move, and since he can clone his board state, the overall setup provides a
brute-force opportunity for the amateurs to win.

Breaking from the metaphor now, the goal is to spot which positions in the
genome have undergone clonal expansion, and when.  This is done by comparing
the measured base composition of different cancer tissue samples at successive
timepoints.  And to get an accurate picture, one must be able to detect rare
subpopulations.


So the problem is this:  Now imagine I give you an urn with a billion balls,
some unknown fraction of red, blue, green, and yellow.  You can only take out
5000 balls, and you actually can't observe the color of each ball directly.
You have a machine that can measure the color, but it is not 100% accurate.  In
fact, it outputs a color and a confidence score (based on its own internal
metrics) telling you how likely it is to be correct.  For the sake of argument,
let's say we can take this confidence score as accurate.

So you get your 5000 measurements of (color, confidence score), and now you
want to estimate the fraction of different colors in the full urn.  The model I chose
is a two-stage model in which: 



Technical problems solved:

1. Multi-threaded, multi-file coordinated filter with fixed memory limit.

The problem: Given a set of files, each containing records in order, apply a
function to corresponding records from each file.  Output the result in the
same order as the input.  By "corresponding records", I mean that each record
has a logical coordinate that is different from its physical coordinate in the
file.  For instance, if a record is "a single stock trade", then the logical
coordinate might be a timestamp.  Then, records in two different files would
correspond if their timestamps both fell within the same time interval.  This
might mean that two different files had different numbers of records in a
logical range.   (For instance, if aech file represpents stock trades for a
particular stock, then a more actively traded stock will have more records in a
given hour, say)

We build up the solution from simpler concepts as follows:

Single-threaded, single-file filter:
 
The classic UNIX-style "filter" on one file simply reads "records" of a file in
order, applies some function to each set of records, and outputs the result in
the same order.  It can thus work within the amount of memory needed to hold
the set of records.

Multi-threaded, single-file filter:

Now, for N threads, if the program reads in N sets of records, each thread can
then invoke the function on that set.  Unfortunately, some sets may take
longer to crunch than others.  In the worst case, the first set takes the
longest.  Since we want to the output to reflect the input order, we must wait
until the first set is crunched before writing to the output file.
During this time, other threads starve.

To fix this, the only solution is to allow more than N sets to be in memory
at a given time, and allow each thread to start working on the next available
set.  And, maintain a pool of output buffers, each populated with output from
one thread, and freed when they offload their contents to the output file.

Multi-threaded, multi-file filter:

Very similar to the single-file case.  We stipulate now that the function being
applied now operates on sets of records in the same logical range (think:
trading data in the same time interval, for multiple stocks).  The thread pool
and output buffer pool design is the same.  The added challenge is that, in
order to stay within memory limits, we must adjust the logical range size so
that the total memory needed for holding that range in memory for all files is
within the required total memory.  That requires some form of scanning,
sampling, or indexing of the input.







It processes very large (TB range) volumes of high
genome sequence data sequenced from viral or cancer samples.  

that it tries to estimate is



the population base composition at each position in the viral or cancer genome
(e.g.  


 in the
form of short 'reads' aligned to the genome   The main technical hurdles it
solves are: 1) filtering very high volumes of data (TB range) given limited
memory, and 2) efficient use of POSIX threads and mutexes.  The  
